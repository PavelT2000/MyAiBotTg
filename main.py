import logging
import asyncio
from io import BytesIO

from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.types import Message
from aiogram.utils.keyboard import ReplyKeyboardBuilder
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from sqlalchemy.sql import text

from config import config
from database import init_db, save_value_to_db
from services import OpenAIService

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
dp = Dispatcher()

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
engine, async_session = init_db(config.DATABASE_URL)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ OpenAI
openai_service = OpenAIService(config.OPENAI_API_KEY, config.AMPLITUDE_API_KEY)

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
def get_main_keyboard():
    builder = ReplyKeyboardBuilder()
    builder.button(text="–ü–æ–º–æ—â—å")
    builder.button(text="–û –±–æ—Ç–µ")
    builder.button(text="–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏")
    builder.button(text="–ú–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ")  # –ù–æ–≤–∞—è –∫–Ω–æ–ø–∫–∞
    return builder.as_markup(resize_keyboard=True)

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ –æ —Ü–µ–Ω–Ω–æ—Å—Ç—è—Ö
class ValuesState(StatesGroup):
    waiting_for_value = State()

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message(Command("start"))
async def start_handler(message: Message):
    logger.info("start handler used")
    openai_service.send_amplitude_event("start_command", str(message.from_user.id))
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏ —è –æ—Ç–≤–µ—á—É –≥–æ–ª–æ—Å–æ–º! –ò—Å–ø–æ–ª—å–∑—É–π /values –¥–ª—è —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–ª–∏ /mood –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ —Ñ–æ—Ç–æ.",
        reply_markup=get_main_keyboard()
    )

@dp.message(Command("mood"))
async def mood_handler(message: Message):
    logger.info("mood handler used")
    openai_service.send_amplitude_event("mood_command", str(message.from_user.id))
    await message.answer("–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ —Å–≤–æ–µ–≥–æ –ª–∏—Ü–∞, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ!")

@dp.message(F.photo)
async def photo_handler(message: Message):
    logger.info("photo handler used")
    try:
        photo = message.photo[-1]  # –ë–µ—Ä—ë–º —Ñ–æ—Ç–æ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º
        file = await bot.get_file(photo.file_id)
        file_url = f"https://api.telegram.org/file/bot{config.TELEGRAM_BOT_TOKEN}/{file.file_path}"
        
        mood = await openai_service.analyze_mood(file_url, message.from_user.id)
        openai_service.send_amplitude_event("photo_processed", str(message.from_user.id), {"mood": mood})

        # –û–∑–≤—É—á–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç
        speech = await openai_service.client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=f"–í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}"
        )
        await message.answer_voice(
            types.BufferedInputFile((await speech.aread()), filename="mood_response.mp3")
        )
        await message.answer(f"ü§ñ –í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ: {e}", exc_info=True)
        openai_service.send_amplitude_event("photo_error", str(message.from_user.id), {"error": str(e)})
        await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

@dp.message(Command("values"))
async def values_handler(message: Message, state: FSMContext):
    logger.info("values handler used")
    openai_service.send_amplitude_event("values_command", str(message.from_user.id))
    await state.set_state(ValuesState.waiting_for_value)
    thread = await openai_service.client.beta.threads.create()
    await state.update_data(thread_id=thread.id)
    await message.answer("–ß—Ç–æ –¥–ª—è —Ç–µ–±—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏? –ù–∞–∑–æ–≤–∏ –æ–¥–Ω—É —Ü–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ –æ–ø–∏—à–∏, —á—Ç–æ —Ç—ã —Ü–µ–Ω–∏—à—å.")

@dp.message(ValuesState.waiting_for_value, F.text | F.voice)
async def process_value(message: Message, state: FSMContext, assistant_id: str):
    try:
        user_question = ""
        event_properties = {}
        if message.voice:
            voice_file = await bot.get_file(message.voice.file_id)
            voice_data = await bot.download_file(voice_file.file_path)
            transcript = await openai_service.client.audio.transcriptions.create(
                file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
                model="whisper-1"
            )
            user_question = transcript.text
            logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {user_question}")
            await message.answer(f"üé§ –í–∞—à –æ—Ç–≤–µ—Ç: {user_question}")
            event_properties["transcript"] = user_question
        else:
            user_question = message.text
            event_properties["text"] = user_question

        openai_service.send_amplitude_event("value_input", str(message.from_user.id), event_properties)

        data = await state.get_data()
        thread_id = data.get("thread_id")

        await openai_service.client.beta.threads.messages.create(
            thread_id=thread_id,
            role="user",
            content=user_question
        )

        run = await openai_service.client.beta.threads.runs.create_and_poll(
            thread_id=thread_id,
            assistant_id=assistant_id
        )

        if run.status == "requires_action" and run.required_action and run.required_action.submit_tool_outputs:
            logger.info(f"–°—Ç–∞—Ç—É—Å requires_action, tool_calls: {run.required_action.submit_tool_outputs.tool_calls}")
            value, error = await openai_service.handle_tool_outputs(thread_id, run)
            if value:
                async with async_session() as session:
                    success, response = await save_value_to_db(session, message.from_user.id, value)
                    await openai_service.submit_tool_output(thread_id, run.id, run.required_action.submit_tool_outputs.tool_calls[0].id, success, response)
                    await message.answer(response)
                    openai_service.send_amplitude_event("value_saved", str(message.from_user.id), {"value": value, "success": success})
                    if success:
                        await state.clear()
            elif error:
                await message.answer(error)
                openai_service.send_amplitude_event("value_error", str(message.from_user.id), {"error": error})
                await state.clear()

        elif run.status != "completed":
            raise Exception(f"Run –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π, —Å—Ç–∞—Ç—É—Å: {run.status}")

        messages = await openai_service.client.beta.threads.messages.list(thread_id=thread_id)
        for msg in messages.data:
            if msg.role == "assistant" and msg.content[0].type == "text":
                assistant_response = msg.content[0].text.value
                logger.info(f"–û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {assistant_response}")
                await message.answer(assistant_response)
                openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": assistant_response})

        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à—É —Ü–µ–Ω–Ω–æ—Å—Ç—å.")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {e}", exc_info=True)
        openai_service.send_amplitude_event("value_processing_error", str(message.from_user.id), {"error": str(e)})
        await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        await state.clear()

@dp.message(F.text)
async def text_handler(message: Message, state: FSMContext):
    logger.info("text handler used")
    event_properties = {"text": message.text.lower()}
    openai_service.send_amplitude_event("text_message", str(message.from_user.id), event_properties)
    if message.text.lower() == "–ø–æ–º–æ—â—å":
        await message.answer("–û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π /values –¥–ª—è —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–ª–∏ /mood –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è.")
    elif message.text.lower() == "–æ –±–æ—Ç–µ":
        await message.answer("–Ø –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–∞ OpenAI API —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è.")
    elif message.text.lower() == "–º–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏":
        async with async_session() as session:
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–ª—è user_id: {message.from_user.id}")
                result = await session.execute(
                    text("SELECT value FROM user_values WHERE user_id = :user_id"),
                    {"user_id": message.from_user.id}
                )
                values = result.fetchall()
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è user_id {message.from_user.id}: {values}")
                if values:
                    values_list = [row[0] for row in values]
                    await message.answer(f"–í–∞—à–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(values_list)}")
                    openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": values_list})
                else:
                    await message.answer("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /values, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Ö.")
                    openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": []})
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–ª—è user_id {message.from_user.id}: {e}", exc_info=True)
                openai_service.send_amplitude_event("values_error", str(message.from_user.id), {"error": str(e)})
                await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–∞—à–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
    elif message.text.lower() == "–º–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ":
        await mood_handler(message)
    else:
        await message.answer("–ò—Å–ø–æ–ª—å–∑—É–π –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, /values –∏–ª–∏ /mood.")

@dp.message(F.voice)
async def voice_handler(message: Message, assistant_id: str):
    logger.info("voice handler used")
    try:
        voice_file = await bot.get_file(message.voice.file_id)
        voice_data = await bot.download_file(voice_file.file_path)
        
        transcript = await openai_service.client.audio.transcriptions.create(
            file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
            model="whisper-1"
        )
        user_question = transcript.text
        await message.answer(f"üé§ –í–∞—à –≤–æ–ø—Ä–æ—Å: {user_question}")
        openai_service.send_amplitude_event("voice_message", str(message.from_user.id), {"transcript": user_question})

        thread = await openai_service.client.beta.threads.create()
        await openai_service.client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=user_question
        )
        
        run = await openai_service.client.beta.threads.runs.create_and_poll(
            thread_id=thread.id,
            assistant_id=assistant_id
        )
        
        if run.status == "requires_action" and run.required_action and run.required_action.submit_tool_outputs:
            logger.info(f"–°—Ç–∞—Ç—É—Å requires_action, tool_calls: {run.required_action.submit_tool_outputs.tool_calls}")
            value, error = await openai_service.handle_tool_outputs(thread.id, run)
            if value:
                async with async_session() as session:
                    success, response = await save_value_to_db(session, message.from_user.id, value)
                    await openai_service.submit_tool_output(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls[0].id, success, response)
                    await message.answer(response)
                    openai_service.send_amplitude_event("value_saved", str(message.from_user.id), {"value": value, "success": success})
            elif error:
                await message.answer(error)
                openai_service.send_amplitude_event("value_error", str(message.from_user.id), {"error": error})

        elif run.status != "completed":
            raise Exception(f"Run –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π, —Å—Ç–∞—Ç—É—Å: {run.status}")
        
        messages = await openai_service.client.beta.threads.messages.list(thread_id=thread.id)
        assistant_response = next(m.content[0].text.value for m in messages.data if m.role == "assistant")

        speech = await openai_service.client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=assistant_response
        )
        
        await message.answer_voice(
            types.BufferedInputFile((await speech.aread()), filename="response.mp3")
        )
        await message.answer(f"ü§ñ –û—Ç–≤–µ—Ç: {assistant_response}")
        openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": assistant_response})

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {e}", exc_info=True)
        openai_service.send_amplitude_event("voice_error", str(message.from_user.id), {"error": str(e)})
        await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")

async def main():
    global openai_service
    assistant_id = await openai_service.verify_or_create_assistant(config.ASSISTANT_ID)
    if assistant_id != config.ASSISTANT_ID:
        logger.info(f"–û–±–Ω–æ–≤–ª—ë–Ω ASSISTANT_ID —Å {config.ASSISTANT_ID} –Ω–∞ {assistant_id}")
    # –ü–µ—Ä–µ–¥–∞—ë–º assistant_id –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    dp.message.register(lambda message: process_value(message, state=message.state, assistant_id=assistant_id), ValuesState.waiting_for_value, F.text | F.voice)
    dp.message.register(lambda message: voice_handler(message, assistant_id=assistant_id), F.voice)
    await dp.start_polling(bot, drop_pending_updates=True)

if __name__ == "__main__":
    asyncio.run(main())
import logging
from io import BytesIO
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.types import Message
from aiogram.utils.keyboard import ReplyKeyboardBuilder
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from config import config
from database import get_user_values, AsyncSession
from services import OpenAIService

logger = logging.getLogger(__name__)

# –û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –±–æ—Ç–∞
BOT_FUNCTIONS = """
ü§ñ **–û –º–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö**:

1. **–ì–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫**: –û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏ —è –ø—Ä–µ–æ–±—Ä–∞–∑—É—é –µ–≥–æ –≤ —Ç–µ–∫—Å—Ç, –æ—Ç–≤–µ—á—É –Ω–∞ —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å (–≤–∫–ª—é—á–∞—è –≤–æ–ø—Ä–æ—Å—ã –æ —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞) –∏ –æ–∑–≤—É—á—É –æ—Ç–≤–µ—Ç!
2. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π**: –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É `/values`, —á—Ç–æ–±—ã –æ–±—Å—É–¥–∏—Ç—å, —á—Ç–æ –¥–ª—è —Ç–µ–±—è –≤–∞–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–µ–º—å—è, —Å–≤–æ–±–æ–¥–∞, —É—Å–ø–µ—Ö).
3. **–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è**: –û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ —Å –∫–æ–º–∞–Ω–¥–æ–π `/mood`, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ.
4. **–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏**: –ù–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É "–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏", —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏.

–ü–æ–ø—Ä–æ–±—É–π –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏! –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å, –Ω–∞–ø–∏—à–∏ "–ü–æ–º–æ—â—å" –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π `/start`.
"""

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
def get_main_keyboard():
    builder = ReplyKeyboardBuilder()
    builder.button(text="–ü–æ–º–æ—â—å")
    builder.button(text="–û –±–æ—Ç–µ")
    builder.button(text="–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏")
    builder.button(text="–ú–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ")
    return builder.as_markup(resize_keyboard=True)

# –°–æ—Å—Ç–æ—è–Ω–∏—è
class ValuesState(StatesGroup):
    waiting_for_value = State()

class GeneralState(StatesGroup):
    conversation = State()

def register_handlers(dp: Dispatcher, bot: Bot, openai_service: OpenAIService, assistant_id: str, async_session):
    @dp.message(Command("start"))
    async def start_handler(message: Message, state: FSMContext):
        logger.info("start handler used")
        openai_service.send_amplitude_event("start_command", str(message.from_user.id))
        await state.clear()
        await message.answer(
            f"–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π —É–º–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. üòä\n\n{BOT_FUNCTIONS}",
            reply_markup=get_main_keyboard(),
            parse_mode="Markdown"
        )

    @dp.message(Command("mood"))
    async def mood_handler(message: Message):
        logger.info("mood handler used")
        openai_service.send_amplitude_event("mood_command", str(message.from_user.id))
        await message.answer("–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ —Å–≤–æ–µ–≥–æ –ª–∏—Ü–∞, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ!")

    @dp.message(F.photo)
    async def photo_handler(message: Message):
        logger.info("photo handler used")
        try:
            photo = message.photo[-1]
            file = await bot.get_file(photo.file_id)
            file_url = f"https://api.telegram.org/file/bot{config.TELEGRAM_BOT_TOKEN}/{file.file_path}"
            mood = await openai_service.analyze_mood(file_url, message.from_user.id)
            openai_service.send_amplitude_event("photo_processed", str(message.from_user.id), {"mood": mood})
            speech = await openai_service.client.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=f"–í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}"
            )
            await message.answer_voice(
                types.BufferedInputFile((await speech.aread()), filename="mood_response.mp3")
            )
            await message.answer(f"ü§ñ –í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ: {e}", exc_info=True)
            openai_service.send_amplitude_event("photo_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

    @dp.message(Command("values"))
    async def values_handler(message: Message, state: FSMContext):
        logger.info("values handler used")
        openai_service.send_amplitude_event("values_command", str(message.from_user.id))
        await state.set_state(ValuesState.waiting_for_value)
        thread = await openai_service.client.beta.threads.create()
        await state.update_data(thread_id=thread.id)
        await message.answer("–ß—Ç–æ –¥–ª—è —Ç–µ–±—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏? –ù–∞–∑–æ–≤–∏ –æ–¥–Ω—É —Ü–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ –æ–ø–∏—à–∏, —á—Ç–æ —Ç—ã —Ü–µ–Ω–∏—à—å.")

    @dp.message(ValuesState.waiting_for_value, F.text | F.voice)
    async def process_value(message: Message, state: FSMContext):
        try:
            user_input = ""
            event_properties = {}
            if message.voice:
                voice_file = await bot.get_file(message.voice.file_id)
                voice_data = await bot.download_file(voice_file.file_path)
                transcript = await openai_service.client.audio.transcriptions.create(
                    file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
                    model="whisper-1"
                )
                user_input = transcript.text
                await message.answer(f"üé§ –í–∞—à –æ—Ç–≤–µ—Ç: {user_input}")
                event_properties["transcript"] = user_input
            else:
                user_input = message.text
                event_properties["text"] = user_input

            openai_service.send_amplitude_event("value_input", str(message.from_user.id), event_properties)
            data = await state.get_data()
            thread_id = data.get("thread_id")
            await openai_service.client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=user_input
            )
            response, error = await openai_service.process_thread(thread_id, assistant_id)
            if error:
                await message.answer(error)
                openai_service.send_amplitude_event("value_error", str(message.from_user.id), {"error": error})
                return
            if response and "–¶–µ–Ω–Ω–æ—Å—Ç—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞" in response:
                await state.clear()
            await message.answer(response)
            openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": response})
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {e}", exc_info=True)
            openai_service.send_amplitude_event("value_processing_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
            await state.clear()

    @dp.message(F.text)
    async def text_handler(message: Message, state: FSMContext):
        logger.info("text handler used")
        event_properties = {"text": message.text.lower()}
        openai_service.send_amplitude_event("text_message", str(message.from_user.id), event_properties)
        if message.text.lower() == "–ø–æ–º–æ—â—å":
            await message.answer("–û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π /values –¥–ª—è —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–ª–∏ /mood –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è.")
        elif message.text.lower() == "–æ –±–æ—Ç–µ":
            await message.answer(BOT_FUNCTIONS, parse_mode="Markdown")
        elif message.text.lower() == "–º–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏":
            async with async_session() as session:
                try:
                    values = await get_user_values(session, message.from_user.id)
                    if values:
                        await message.answer(f"–í–∞—à–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(values)}")
                        openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": values})
                    else:
                        await message.answer("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /values.")
                        openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": []})
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π: {e}", exc_info=True)
                    openai_service.send_amplitude_event("values_error", str(message.from_user.id), {"error": str(e)})
                    await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π.")
        elif message.text.lower() == "–º–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ":
            await mood_handler(message)
        else:
            await state.set_state(GeneralState.conversation)
            data = await state.get_data()
            thread_id = data.get("thread_id")
            if not thread_id:
                thread = await openai_service.client.beta.threads.create()
                thread_id = thread.id
                await state.update_data(thread_id=thread_id)
            await openai_service.client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=message.text
            )
            response, error = await openai_service.process_thread(thread_id, assistant_id)
            if error:
                await message.answer(error)
            else:
                await message.answer(response)
                speech = await openai_service.client.audio.speech.create(
                    model="tts-1",
                    voice="alloy",
                    input=response
                )
                await message.answer_voice(
                    types.BufferedInputFile((await speech.aread()), filename="response.mp3")
                )
            openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": response or error})

    @dp.message(F.voice)
    async def voice_handler(message: Message, state: FSMContext):
        logger.info("voice handler used")
        try:
            voice_file = await bot.get_file(message.voice.file_id)
            voice_data = await bot.download_file(voice_file.file_path)
            transcript = await openai_service.client.audio.transcriptions.create(
                file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
                model="whisper-1"
            )
            user_question = transcript.text
            await message.answer(f"üé§ –í–∞—à –≤–æ–ø—Ä–æ—Å: {user_question}")
            openai_service.send_amplitude_event("voice_message", str(message.from_user.id), {"transcript": user_question})
            data = await state.get_data()
            thread_id = data.get("thread_id")
            if not thread_id:
                thread = await openai_service.client.beta.threads.create()
                thread_id = thread.id
                await state.update_data(thread_id=thread.id)
            await openai_service.client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=user_question
            )
            response, error = await openai_service.process_thread(thread_id, assistant_id)
            if error:
                await message.answer(error)
            else:
                await message.answer(response)
                speech = await openai_service.client.audio.speech.create(
                    model="tts-1",
                    voice="alloy",
                    input=response
                )
                await message.answer_voice(
                    types.BufferedInputFile((await speech.aread()), filename="response.mp3")
                )
            openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": response or error})
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}", exc_info=True)
            openai_service.send_amplitude_event("voice_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")
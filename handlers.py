import logging
from io import BytesIO

from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.types import Message
from aiogram.utils.keyboard import ReplyKeyboardBuilder
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup

from config import config
from database import get_user_values, AsyncSession  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ get_user_values –∏ AsyncSession
from services import OpenAIService

logger = logging.getLogger(__name__)

# –û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –±–æ—Ç–∞
BOT_FUNCTIONS = """
ü§ñ **–û –º–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö**:

1. **–ì–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫**: –û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏ —è –ø—Ä–µ–æ–±—Ä–∞–∑—É—é –µ–≥–æ –≤ —Ç–µ–∫—Å—Ç, –æ—Ç–≤–µ—á—É –Ω–∞ —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å –∏ –æ–∑–≤—É—á—É –æ—Ç–≤–µ—Ç! –ò—Å–ø–æ–ª—å–∑—É—é –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ OpenAI –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–æ–ª–æ—Å–∞.
2. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π**: –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É `/values`, —á—Ç–æ–±—ã –æ–±—Å—É–¥–∏—Ç—å, —á—Ç–æ –¥–ª—è —Ç–µ–±—è –≤–∞–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–µ–º—å—è, —Å–≤–æ–±–æ–¥–∞, —É—Å–ø–µ—Ö). –Ø —Å–æ—Ö—Ä–∞–Ω—é —Ç–≤–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏.
3. **–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è**: –û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ —Å–≤–æ–µ–≥–æ –ª–∏—Ü–∞ —Å –∫–æ–º–∞–Ω–¥–æ–π `/mood`, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (—Ä–∞–¥–æ—Å—Ç—å, –≥—Ä—É—Å—Ç—å, —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ –∏ —Ç.–¥.) —Å –ø–æ–º–æ—â—å—é –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
4. **–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏**: –ù–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É "–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏", —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π.

–ü–æ–ø—Ä–æ–±—É–π –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏! –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å, –Ω–∞–ø–∏—à–∏ "–ü–æ–º–æ—â—å" –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É `/start`.
"""

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
def get_main_keyboard():
    builder = ReplyKeyboardBuilder()
    builder.button(text="–ü–æ–º–æ—â—å")
    builder.button(text="–û –±–æ—Ç–µ")
    builder.button(text="–ú–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏")
    builder.button(text="–ú–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ")
    return builder.as_markup(resize_keyboard=True)

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ –æ —Ü–µ–Ω–Ω–æ—Å—Ç—è—Ö
class ValuesState(StatesGroup):
    waiting_for_value = State()

def register_handlers(dp: Dispatcher, bot: Bot, openai_service: OpenAIService, assistant_id: str, async_session):  # –î–æ–±–∞–≤–ª–µ–Ω async_session
    @dp.message(Command("start"))
    async def start_handler(message: Message):
        logger.info("start handler used")
        openai_service.send_amplitude_event("start_command", str(message.from_user.id))
        await message.answer(
            f"–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π —É–º–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. üòä\n\n{BOT_FUNCTIONS}",
            reply_markup=get_main_keyboard(),
            parse_mode="Markdown"
        )

    @dp.message(Command("mood"))
    async def mood_handler(message: Message):
        logger.info("mood handler used")
        openai_service.send_amplitude_event("mood_command", str(message.from_user.id))
        await message.answer("–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ —Å–≤–æ–µ–≥–æ –ª–∏—Ü–∞, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ!")

    @dp.message(F.photo)
    async def photo_handler(message: Message):
        logger.info("photo handler used")
        try:
            photo = message.photo[-1]
            file = await bot.get_file(photo.file_id)
            file_url = f"https://api.telegram.org/file/bot{config.TELEGRAM_BOT_TOKEN}/{file.file_path}"
            
            mood = await openai_service.analyze_mood(file_url, message.from_user.id)
            openai_service.send_amplitude_event("photo_processed", str(message.from_user.id), {"mood": mood})

            speech = await openai_service.client.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=f"–í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}"
            )
            await message.answer_voice(
                types.BufferedInputFile((await speech.aread()), filename="mood_response.mp3")
            )
            await message.answer(f"ü§ñ –í–∞—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ: {e}", exc_info=True)
            openai_service.send_amplitude_event("photo_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

    @dp.message(Command("values"))
    async def values_handler(message: Message, state: FSMContext):
        logger.info("values handler used")
        openai_service.send_amplitude_event("values_command", str(message.from_user.id))
        await state.set_state(ValuesState.waiting_for_value)
        thread = await openai_service.client.beta.threads.create()
        await state.update_data(thread_id=thread.id)
        await message.answer("–ß—Ç–æ –¥–ª—è —Ç–µ–±—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–æ –≤ –∂–∏–∑–Ω–∏? –ù–∞–∑–æ–≤–∏ –æ–¥–Ω—É —Ü–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ –æ–ø–∏—à–∏, —á—Ç–æ —Ç—ã —Ü–µ–Ω–∏—à—å.")

    @dp.message(ValuesState.waiting_for_value, F.text | F.voice)
    async def process_value(message: Message, state: FSMContext):
        try:
            user_question = ""
            event_properties = {}
            if message.voice:
                voice_file = await bot.get_file(message.voice.file_id)
                voice_data = await bot.download_file(voice_file.file_path)
                transcript = await openai_service.client.audio.transcriptions.create(
                    file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
                    model="whisper-1"
                )
                user_question = transcript.text
                logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {user_question}")
                await message.answer(f"üé§ –í–∞—à –æ—Ç–≤–µ—Ç: {user_question}")
                event_properties["transcript"] = user_question
            else:
                user_question = message.text
                event_properties["text"] = user_question

            openai_service.send_amplitude_event("value_input", str(message.from_user.id), event_properties)

            data = await state.get_data()
            thread_id = data.get("thread_id")

            await openai_service.client.beta.threads.messages.create(
                thread_id=thread_id,
                role="user",
                content=user_question
            )

            run = await openai_service.client.beta.threads.runs.create_and_poll(
                thread_id=thread_id,
                assistant_id=assistant_id
            )

            if run.status == "requires_action" and run.required_action and run.required_action.submit_tool_outputs:
                async with async_session() as session:
                    response, success = await openai_service.process_tool_call(thread_id, run, session, message.from_user.id)
                    await message.answer(response)
                    openai_service.send_amplitude_event(
                        "value_saved" if success else "value_error",
                        str(message.from_user.id),
                        {"value": response, "success": success} if success else {"error": response}
                    )
                    if success:
                        await state.clear()
                return
            elif run.status != "completed":
                raise Exception(f"Run –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π, —Å—Ç–∞—Ç—É—Å: {run.status}")

            messages = await openai_service.client.beta.threads.messages.list(thread_id=thread_id)
            for msg in messages.data:
                if msg.role == "assistant" and msg.content[0].type == "text":
                    assistant_response = msg.content[0].text.value
                    logger.info(f"–û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {assistant_response}")
                    await message.answer(assistant_response)
                    openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": assistant_response})

            await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à—É —Ü–µ–Ω–Ω–æ—Å—Ç—å.")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {e}", exc_info=True)
            openai_service.send_amplitude_event("value_processing_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
            await state.clear()

    @dp.message(F.text)
    async def text_handler(message: Message, state: FSMContext):
        logger.info("text handler used")
        event_properties = {"text": message.text.lower()}
        openai_service.send_amplitude_event("text_message", str(message.from_user.id), event_properties)
        if message.text.lower() == "–ø–æ–º–æ—â—å":
            await message.answer("–û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π /values –¥–ª—è —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–ª–∏ /mood –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è.")
        elif message.text.lower() == "–æ –±–æ—Ç–µ":
            await message.answer(BOT_FUNCTIONS, parse_mode="Markdown")
        elif message.text.lower() == "–º–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏":
            async with async_session() as session:
                try:
                    logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–ª—è user_id: {message.from_user.id}")
                    values = await get_user_values(session, message.from_user.id)
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è user_id {message.from_user.id}: {values}")
                    if values:
                        await message.answer(f"–í–∞—à–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(values)}")
                        openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": values})
                    else:
                        await message.answer("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /values, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Ö.")
                        openai_service.send_amplitude_event("values_viewed", str(message.from_user.id), {"values": []})
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–ª—è user_id {message.from_user.id}: {e}", exc_info=True)
                    openai_service.send_amplitude_event("values_error", str(message.from_user.id), {"error": str(e)})
                    await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–∞—à–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        elif message.text.lower() == "–º–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ":
            await mood_handler(message)
        else:
            await message.answer("–ò—Å–ø–æ–ª—å–∑—É–π –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, /values –∏–ª–∏ /mood.")

    @dp.message(F.voice)
    async def voice_handler(message: Message):
        logger.info("voice handler used")
        try:
            voice_file = await bot.get_file(message.voice.file_id)
            voice_data = await bot.download_file(voice_file.file_path)
            
            transcript = await openai_service.client.audio.transcriptions.create(
                file=("voice.ogg", BytesIO(voice_data.read()), "audio/ogg"),
                model="whisper-1"
            )
            user_question = transcript.text
            await message.answer(f"üé§ –í–∞—à –≤–æ–ø—Ä–æ—Å: {user_question}")
            openai_service.send_amplitude_event("voice_message", str(message.from_user.id), {"transcript": user_question})

            thread = await openai_service.client.beta.threads.create()
            await openai_service.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=user_question
            )
            
            run = await openai_service.client.beta.threads.runs.create_and_poll(
                thread_id=thread.id,
                assistant_id=assistant_id
            )
            
            if run.status == "requires_action" and run.required_action and run.required_action.submit_tool_outputs:
                async with async_session() as session:
                    response, success = await openai_service.process_tool_call(thread.id, run, session, message.from_user.id)
                    await message.answer(response)
                    openai_service.send_amplitude_event(
                        "value_saved" if success else "value_error",
                        str(message.from_user.id),
                        {"value": response, "success": success} if success else {"error": response}
                    )
                    return
            elif run.status != "completed":
                raise Exception(f"Run –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π, —Å—Ç–∞—Ç—É—Å: {run.status}")
            
            messages = await openai_service.client.beta.threads.messages.list(thread_id=thread.id)
            assistant_response = next(m.content[0].text.value for m in messages.data if m.role == "assistant")

            speech = await openai_service.client.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=assistant_response
            )
            
            await message.answer_voice(
                types.BufferedInputFile((await speech.aread()), filename="response.mp3")
            )
            await message.answer(f"ü§ñ –û—Ç–≤–µ—Ç: {assistant_response}")
            openai_service.send_amplitude_event("assistant_response", str(message.from_user.id), {"response": assistant_response})

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}", exc_info=True)
            openai_service.send_amplitude_event("voice_error", str(message.from_user.id), {"error": str(e)})
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")